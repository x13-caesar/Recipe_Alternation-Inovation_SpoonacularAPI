{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "NLP_final_PreProcessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5YNRfp3WOEA2"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hB5cntrzksNO"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oEFHr_DTdIu6"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('./data/nlp_finalproj_data_withids.csv')\n",
        "data.drop(['keto', 'ingred_id'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "colab_type": "code",
        "id": "8lb3B42I5hmc",
        "outputId": "dee30845-6c20-4bb0-e112-2d6113f93bb8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Chinese           266\n",
              "Caribbean         250\n",
              "American          250\n",
              "German            250\n",
              "Middle Eastern    250\n",
              "Thai              250\n",
              "French            250\n",
              "Mexican           250\n",
              "Vietnamese        250\n",
              "Mediterranean     250\n",
              "Italian           250\n",
              "Japanese          249\n",
              "Korean            241\n",
              "Indian            211\n",
              "African           123\n",
              "Name: cuisine, dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['cuisine'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nwOCcOWTkybw"
      },
      "source": [
        "# Encode and seperate features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "colab_type": "code",
        "id": "QIygOuolB-no",
        "outputId": "f0847ca8-d433-4a25-9735-dbe5a1361f07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import re\n",
        "\n",
        "from sklearn import preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "GGsAmmm2NtjM"
      },
      "outputs": [],
      "source": [
        "def deduplicate(ingredient):\n",
        "    # try to uniform the duplicated or similar ingredients\n",
        "    ingredient = re.sub('canned |fresh |cooked |dried |dry |ground |refrigerated |light |ready-to-use ', '', ingredient)\n",
        "    ingredient = re.sub('lettuc[^0-9]+', 'lettuc', ingredient)\n",
        "    ingredient = re.sub('[^0-9]+ salt', 'salt', ingredient)\n",
        "    ingredient = re.sub('[^0-9]+ basil|basil [^0-9]+', 'basil', ingredient)\n",
        "    ingredient = re.sub('[^0-9]+ bacon|bacon [^0-9]+', 'bacon', ingredient)\n",
        "    ingredient = re.sub('[^0-9]+ thyme|thyme [^0-9]+', 'thyme', ingredient)\n",
        "    ingredient = re.sub('[^0-9]+ cilantro|cilantro [^0-9]+', 'cilantro', ingredient)\n",
        "    ingredient = re.sub('[^0-9]+ parsley|parsley [^0-9]+', 'parsley', ingredient)\n",
        "    ingredient = re.sub('[^0-9]+ cinnamon|cinnamon [^0-9]+', 'cinnamon', ingredient)\n",
        "    ingredient = re.sub('cream[^0-9]+chees|cream chees[^0-9]+', 'cream chees', ingredient)\n",
        "    ingredient = re.sub('[^0-9]+parmesan[^0-9]+', 'parmesan chees', ingredient)\n",
        "    ingredient = re.sub('[^0-9]+ peanut butt', 'peanut butt', ingredient)\n",
        "    ingredient = re.sub('[^0-9]+ chili pepper', 'chili pepp', ingredient)\n",
        "    ingredient = re.sub('[^0-9]+ nectar', 'nectar', ingredient)\n",
        "    ingredient = re.sub('[^0-9]+ tofu', 'tofu', ingredient)\n",
        "    ingredient = re.sub('[^0-9]+ tomato', 'tomato', ingredient)\n",
        "    ingredient = re.sub('[^0-9]+ bell pepp', 'bell pepp', ingredient)\n",
        "    ingredient = re.sub('egg roll wra[^0-9]+', 'egg roll wrap', ingredient)\n",
        "    return ingredient\n",
        "\n",
        "\n",
        "def clean_data(igd_data):\n",
        "    # igd_data: a array-like object which contains all ingredients list of training dataset\n",
        "    # stem: boolean, default to be True\n",
        "    # *output: a dict\n",
        "    igds = list()\n",
        "    raw_igds = igd_data.split(sep=',')\n",
        "    for i in raw_igds:\n",
        "        igds.append(re.sub(' *\\'|\\[|\\]|\\ *\"', '', i)) # delete unneeded punctuations\n",
        "    return igds\n",
        "\n",
        "\n",
        "def stem_and_deduplicate(igd_data):\n",
        "    ps = PorterStemmer()\n",
        "    stem_igds = [ps.stem(x) for x in igd_data]\n",
        "    return [deduplicate(y) for y in stem_igds]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gUkZHN7qwan6"
      },
      "outputs": [],
      "source": [
        "# Split and encode ingredients\n",
        "all_ingredients = [clean_data(x) for x in data['ingredients']]\n",
        "\n",
        "all_ingredients = [stem_and_deduplicate(clean_data(x)) for x in data['ingredients']]\n",
        "\n",
        "le_igd = preprocessing.LabelEncoder()\n",
        "le_igd.fit(np.hstack(all_ingredients))\n",
        "\n",
        "data['encoded_ingredients'] = [le_igd.transform(x) for x in all_ingredients]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "colab_type": "code",
        "id": "Qr2ISuPm7eQA",
        "outputId": "66c7f671-0b30-4d44-b7c4-df0efced29f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " olive oil\n",
            "lightly beaten egg\n",
            "red red curry past\n",
            " sugar\n",
            "cooked uncooked ( 250g-300g ) basmati ric\n",
            "slice spring onion\n",
            "frozen pea\n",
            " soy sauc\n",
            "smoked flak mackerel\n",
            "cut into half moon cucumb\n"
          ]
        }
      ],
      "source": [
        "# Concatenate the ingredients and process\n",
        "\n",
        "# Split the process\n",
        "ps = PorterStemmer()\n",
        "process = list()\n",
        "for i in range(len(data)):\n",
        "    tem = data['process'][i].split(sep=', [')\n",
        "    tokens = [re.sub(' *\\'|\\[|\\]|\\ *\"', '', i).replace(',', ' ') for i in tem]\n",
        "    process.append([ps.stem(x) for x in tokens])\n",
        "\n",
        "# Link them\n",
        "process_igds = []\n",
        "for (n, left) in enumerate(process):\n",
        "    right = all_ingredients[n]\n",
        "    piece = []\n",
        "    for (i, p) in enumerate(left):\n",
        "        piece.append(p+' '+right[i])\n",
        "    process_igds.append(piece)\n",
        "\n",
        "data['process ingredients'] = process_igds\n",
        "\n",
        "print(\"For recipe:\", data['title'][5])\n",
        "print(\"Before pairing:\")\n",
        "print(data['ingredients'][5])\n",
        "print(data['process'][5])\n",
        "\n",
        "tem = data['process ingredients'][5]\n",
        "for t in tem:\n",
        "    print(t)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vSN0HPcXcJ2G"
      },
      "outputs": [],
      "source": [
        "def extract_methods(methods_data, stopwords):\n",
        "    tokens = word_tokenize(str(methods_data).lower())\n",
        "    tagged_ins = nltk.pos_tag(tokens)\n",
        "    methods = set()\n",
        "    ps = PorterStemmer()\n",
        "    for (m, tag) in tagged_ins:\n",
        "        if tag in ['VB', 'NN']: # NOT decided if to take NN in\n",
        "            methods.add(ps.stem(m))\n",
        "    effective_methods = [w for w in methods if not w in stopwords]\n",
        "    return effective_methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "q_LBuXWPNQM8"
      },
      "outputs": [],
      "source": [
        "# Split and encode methods (from instructions)\n",
        "\n",
        "freq_vb = ['put', 'add', 'use', 'keep', 'prepar', 'start']\n",
        "\n",
        "data['instructions'] = [extract_methods(x, freq_vb) for x in data['instructions']]\n",
        "\n",
        "# Encode the instructions words\n",
        "le_methods = preprocessing.LabelEncoder()\n",
        "le_methods.fit(np.hstack(data['instructions']))\n",
        "\n",
        "data['encoded_instructions'] = [le_methods.transform(x) for x in data['instructions']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "colab_type": "code",
        "id": "tNAdZCu5OXOt",
        "outputId": "80df54fb-b19e-434e-e589-d87b1c8b744d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>no diet</th>\n",
              "      <th>dairy fre</th>\n",
              "      <th>fodmap friendli</th>\n",
              "      <th>gluten fre</th>\n",
              "      <th>ketogen</th>\n",
              "      <th>lacto ovo vegetarian</th>\n",
              "      <th>paleolith</th>\n",
              "      <th>pescatarian</th>\n",
              "      <th>primal</th>\n",
              "      <th>vegan</th>\n",
              "      <th>whole 30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3585</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3586</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3587</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3588</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3589</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3590 rows Ã— 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      no diet  dairy fre  fodmap friendli  ...  primal  vegan  whole 30\n",
              "0         0.0        1.0              0.0  ...     0.0    1.0       0.0\n",
              "1         0.0        1.0              0.0  ...     0.0    0.0       0.0\n",
              "2         0.0        1.0              0.0  ...     0.0    0.0       0.0\n",
              "3         0.0        1.0              0.0  ...     0.0    0.0       0.0\n",
              "4         0.0        1.0              0.0  ...     0.0    0.0       0.0\n",
              "...       ...        ...              ...  ...     ...    ...       ...\n",
              "3585      1.0        0.0              0.0  ...     0.0    0.0       0.0\n",
              "3586      0.0        1.0              0.0  ...     0.0    0.0       0.0\n",
              "3587      0.0        0.0              0.0  ...     0.0    0.0       0.0\n",
              "3588      1.0        0.0              0.0  ...     0.0    0.0       0.0\n",
              "3589      0.0        1.0              0.0  ...     0.0    0.0       0.0\n",
              "\n",
              "[3590 rows x 11 columns]"
            ]
          },
          "execution_count": 29,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Split and encode diets\n",
        "\n",
        "all_diets = [stem_and_deduplicate(clean_data(x)) for x in data['diets']]\n",
        "\n",
        "le_diets = preprocessing.LabelEncoder()\n",
        "le_diets.fit(np.hstack(all_diets))\n",
        "le_diets.classes_\n",
        "\n",
        "data['encoded_diets'] = [le_diets.transform(x) for x in all_diets]\n",
        "\n",
        "# Dict for search\n",
        "code2diets = {x:n for n,x in zip(range(10),le_diets.classes_.tolist())}\n",
        "\n",
        "diet_info = np.zeros((len(data), 11))\n",
        "for i in le_diets.classes_.tolist():\n",
        "    for (num, recipe_info) in enumerate(data['encoded_diets']):\n",
        "        for diet in recipe_info:\n",
        "            diet_info[int(num), int(diet)] = 1\n",
        "\n",
        "diet_names = le_diets.classes_.tolist()\n",
        "diet_names[0] = 'no diet'\n",
        "\n",
        "diet_info = pd.DataFrame(diet_info, columns=diet_names)\n",
        "data = data.join(diet_info)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5OaTPajkQbS5"
      },
      "outputs": [],
      "source": [
        "# Seperate nutrition feature into three columns\n",
        "\n",
        "tem = []\n",
        "for n in data['nutritions']:\n",
        "    numberonly = re.sub(' *\\'|\\{|\\}|\\ *\"|[a-z|A-Z]|:', '', n)\n",
        "    ns = re.split(r',', numberonly)\n",
        "    tem.append(ns)\n",
        "\n",
        "data['protein'], data['fat'], data['carbs'] = [float(x[0]) for x in tem], [float(x[1]) for x in tem], [float(x[2]) for x in tem]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OEMFmj4PI0eC"
      },
      "outputs": [],
      "source": [
        "# Save the preprocessed data\n",
        "\n",
        "data.to_csv('./data/nlp_finalproj_data_preprocessed.csv')"
      ]
    }
  ]
}