{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First let's load in our required resources for data loading and model creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pandas as pd\n",
    "import time \n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.optimizers import RMSprop\n",
    "import nltk\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint  \n",
    "sns.set_context()\n",
    "sns.set_style()\n",
    "\n",
    "def seed_everything(SEED):\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    \n",
    "seed_everything(6000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "SEQ_LENGTH = 10\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('./data/nlp_finalproj_data_withids.csv').iloc[:,0:11]\n",
    "# check any ducplicated r_id\n",
    "unique_rid = set(df['r_id'])\n",
    "\n",
    "\n",
    "# remove ducplicated r_id\n",
    "df.drop_duplicates(subset = ['r_id'],keep = 'first',inplace = True)\n",
    "\n",
    "df = df[df['cuisine']=='Chinese']\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs = [instruction for instruction in df['instructions']]\n",
    "# concatenate all the instuctions together\n",
    "docs = [str(instruction).replace('\\n','').replace('\\xa0','') for instruction in df['instructions']]\n",
    "#text = \"\".join(str(instruction) for instruction in df['instructions'])\n",
    "#text = text.replace('\\n','').replace('\\xa0','')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.regexp import (\n",
    "    RegexpTokenizer,\n",
    "    WhitespaceTokenizer,\n",
    "    BlanklineTokenizer,\n",
    "    WordPunctTokenizer,\n",
    "    wordpunct_tokenize,\n",
    "    regexp_tokenize,\n",
    "    blankline_tokenize,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_token = [WordPunctTokenizer().tokenize(x.lower()) for x in docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2573"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = ['the','a','.']\n",
    "corpus = [token for recipe in text_token for token in recipe]\n",
    "corpus = [w for w in corpus if not w in stopwords] \n",
    "vocab = set(corpus)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "In the cells, below, I'm creating a couple **dictionaries** to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# we create two dictionaries:\n",
    "# 1. idx2word\n",
    "# 2. word2idx\n",
    "word = sorted(tuple(vocab))\n",
    "\n",
    "\n",
    "idx2word = dict((i, c) for i, c in enumerate(word))\n",
    "idx2word[len(word)] = '<END>'\n",
    "word2idx = {ww: ii for ii, ww in idx2word.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([word2idx[x] for x in corpus] + [len(word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '!',\n",
       " 1: '!)',\n",
       " 2: '!),',\n",
       " 3: '!).',\n",
       " 4: '!</',\n",
       " 5: '\"',\n",
       " 6: '\")',\n",
       " 7: '\".',\n",
       " 8: '%',\n",
       " 9: '&',\n",
       " 10: \"'\",\n",
       " 11: '(',\n",
       " 12: ')',\n",
       " 13: '),',\n",
       " 14: ').',\n",
       " 15: ');',\n",
       " 16: '*',\n",
       " 17: '**',\n",
       " 18: ',',\n",
       " 19: '-',\n",
       " 20: '--',\n",
       " 21: '.\"',\n",
       " 22: '.)',\n",
       " 23: '.).',\n",
       " 24: '.*',\n",
       " 25: '.**',\n",
       " 26: '.-',\n",
       " 27: '.</',\n",
       " 28: '.•',\n",
       " 29: '/',\n",
       " 30: '0',\n",
       " 31: '00',\n",
       " 32: '000',\n",
       " 33: '012345rating',\n",
       " 34: '1',\n",
       " 35: '10',\n",
       " 36: '103',\n",
       " 37: '109',\n",
       " 38: '11',\n",
       " 39: '12',\n",
       " 40: '12minutes',\n",
       " 41: '13',\n",
       " 42: '13x9',\n",
       " 43: '14',\n",
       " 44: '140',\n",
       " 45: '145',\n",
       " 46: '15',\n",
       " 47: '150',\n",
       " 48: '153',\n",
       " 49: '157',\n",
       " 50: '16',\n",
       " 51: '165',\n",
       " 52: '165f',\n",
       " 53: '17',\n",
       " 54: '177c',\n",
       " 55: '18',\n",
       " 56: '180',\n",
       " 57: '180c',\n",
       " 58: '181',\n",
       " 59: '19',\n",
       " 60: '199',\n",
       " 61: '1991',\n",
       " 62: '1a',\n",
       " 63: '1½',\n",
       " 64: '2',\n",
       " 65: '20',\n",
       " 66: '2000',\n",
       " 67: '2003',\n",
       " 68: '2004',\n",
       " 69: '2005',\n",
       " 70: '2006',\n",
       " 71: '2008',\n",
       " 72: '2009',\n",
       " 73: '200c',\n",
       " 74: '2010',\n",
       " 75: '2011',\n",
       " 76: '2012',\n",
       " 77: '2013',\n",
       " 78: '2017',\n",
       " 79: '20172016',\n",
       " 80: '2017april',\n",
       " 81: '2017author',\n",
       " 82: '2017february',\n",
       " 83: '2017january',\n",
       " 84: '2017march',\n",
       " 85: '2017may',\n",
       " 86: '21',\n",
       " 87: '22',\n",
       " 88: '23',\n",
       " 89: '230',\n",
       " 90: '231',\n",
       " 91: '24',\n",
       " 92: '25',\n",
       " 93: '250',\n",
       " 94: '258',\n",
       " 95: '26',\n",
       " 96: '278',\n",
       " 97: '28',\n",
       " 98: '284',\n",
       " 99: '29',\n",
       " 100: '2a',\n",
       " 101: '2minutes',\n",
       " 102: '2spring',\n",
       " 103: '2½',\n",
       " 104: '3',\n",
       " 105: '30',\n",
       " 106: '300',\n",
       " 107: '31',\n",
       " 108: '32',\n",
       " 109: '325',\n",
       " 110: '325f',\n",
       " 111: '332',\n",
       " 112: '346',\n",
       " 113: '35',\n",
       " 114: '350',\n",
       " 115: '350cut',\n",
       " 116: '350f',\n",
       " 117: '356',\n",
       " 118: '36',\n",
       " 119: '365f',\n",
       " 120: '367',\n",
       " 121: '370',\n",
       " 122: '375',\n",
       " 123: '375f',\n",
       " 124: '38',\n",
       " 125: '3cups',\n",
       " 126: '3½',\n",
       " 127: '4',\n",
       " 128: '40',\n",
       " 129: '400',\n",
       " 130: '400f',\n",
       " 131: '418',\n",
       " 132: '42',\n",
       " 133: '425',\n",
       " 134: '425f',\n",
       " 135: '444',\n",
       " 136: '45',\n",
       " 137: '475',\n",
       " 138: '487',\n",
       " 139: '49calories122',\n",
       " 140: '4months',\n",
       " 141: '4x1',\n",
       " 142: '5',\n",
       " 143: '50',\n",
       " 144: '517',\n",
       " 145: '53',\n",
       " 146: '6',\n",
       " 147: '60',\n",
       " 148: '62',\n",
       " 149: '63',\n",
       " 150: '64',\n",
       " 151: '696',\n",
       " 152: '7',\n",
       " 153: '732',\n",
       " 154: '75',\n",
       " 155: '77',\n",
       " 156: '8',\n",
       " 157: '80',\n",
       " 158: '83',\n",
       " 159: '857',\n",
       " 160: '889',\n",
       " 161: '9',\n",
       " 162: '90',\n",
       " 163: '93',\n",
       " 164: '9x13',\n",
       " 165: ':',\n",
       " 166: '://',\n",
       " 167: ';',\n",
       " 168: '<',\n",
       " 169: '</',\n",
       " 170: '=',\n",
       " 171: '>',\n",
       " 172: '><',\n",
       " 173: '></',\n",
       " 174: '?',\n",
       " 175: 'able',\n",
       " 176: 'about',\n",
       " 177: 'about4',\n",
       " 178: 'above',\n",
       " 179: 'absorb',\n",
       " 180: 'absorbed',\n",
       " 181: 'absorbs',\n",
       " 182: 'accompaniments',\n",
       " 183: 'according',\n",
       " 184: 'accordingly',\n",
       " 185: 'account',\n",
       " 186: 'achilleos',\n",
       " 187: 'across',\n",
       " 188: 'actually',\n",
       " 189: 'ad',\n",
       " 190: 'adapted',\n",
       " 191: 'add',\n",
       " 192: 'added',\n",
       " 193: 'adding',\n",
       " 194: 'additional',\n",
       " 195: 'adheres',\n",
       " 196: 'adjacent',\n",
       " 197: 'adjust',\n",
       " 198: 'admin',\n",
       " 199: 'advertising',\n",
       " 200: 'after',\n",
       " 201: 'afternoon',\n",
       " 202: 'again',\n",
       " 203: 'against',\n",
       " 204: 'agave',\n",
       " 205: 'ago',\n",
       " 206: 'ahead',\n",
       " 207: 'air',\n",
       " 208: 'airtight',\n",
       " 209: 'al',\n",
       " 210: 'alarm',\n",
       " 211: 'all',\n",
       " 212: 'allow',\n",
       " 213: 'allowing',\n",
       " 214: 'almond',\n",
       " 215: 'almonds',\n",
       " 216: 'almost',\n",
       " 217: 'along',\n",
       " 218: 'alongside',\n",
       " 219: 'already',\n",
       " 220: 'also',\n",
       " 221: 'although',\n",
       " 222: 'aluminum',\n",
       " 223: 'always',\n",
       " 224: 'am',\n",
       " 225: 'america',\n",
       " 226: 'american',\n",
       " 227: 'aminos',\n",
       " 228: 'amjalapeo',\n",
       " 229: 'among',\n",
       " 230: 'amount',\n",
       " 231: 'amped',\n",
       " 232: 'an',\n",
       " 233: 'analysis',\n",
       " 234: 'and',\n",
       " 235: 'anise',\n",
       " 236: 'annies',\n",
       " 237: 'another',\n",
       " 238: 'ans',\n",
       " 239: 'antonis',\n",
       " 240: 'any',\n",
       " 241: 'anyone',\n",
       " 242: 'apart',\n",
       " 243: 'app',\n",
       " 244: 'appeared',\n",
       " 245: 'appears',\n",
       " 246: 'appetit',\n",
       " 247: 'appetiteforchina',\n",
       " 248: 'appetizer',\n",
       " 249: 'appetizers',\n",
       " 250: 'apple',\n",
       " 251: 'apples',\n",
       " 252: 'apply',\n",
       " 253: 'approximately',\n",
       " 254: 'apricots',\n",
       " 255: 'archives',\n",
       " 256: 'archives2012',\n",
       " 257: 'archives2013',\n",
       " 258: 'archives2014',\n",
       " 259: 'archives2015',\n",
       " 260: 'are',\n",
       " 261: 'aren',\n",
       " 262: 'aroma',\n",
       " 263: 'aromatic',\n",
       " 264: 'around',\n",
       " 265: 'arrange',\n",
       " 266: 'artichoke',\n",
       " 267: 'artichokes',\n",
       " 268: 'as',\n",
       " 269: 'asafoetida',\n",
       " 270: 'asia',\n",
       " 271: 'asian',\n",
       " 272: 'aside',\n",
       " 273: 'ask',\n",
       " 274: 'asparagus',\n",
       " 275: 'assemble',\n",
       " 276: 'assembled',\n",
       " 277: 'assembling',\n",
       " 278: 'assembly',\n",
       " 279: 'at',\n",
       " 280: 'attached',\n",
       " 281: 'attachment',\n",
       " 282: 'august',\n",
       " 283: 'author',\n",
       " 284: 'available',\n",
       " 285: 'avocado',\n",
       " 286: 'avocados',\n",
       " 287: 'avoid',\n",
       " 288: 'away',\n",
       " 289: 'awhile',\n",
       " 290: 'b',\n",
       " 291: 'baby',\n",
       " 292: 'back',\n",
       " 293: 'backward',\n",
       " 294: 'backyard',\n",
       " 295: 'bacon',\n",
       " 296: 'bad',\n",
       " 297: 'bag',\n",
       " 298: 'bags',\n",
       " 299: 'bake',\n",
       " 300: 'baked',\n",
       " 301: 'baking',\n",
       " 302: 'balanced',\n",
       " 303: 'ball',\n",
       " 304: 'ballantine',\n",
       " 305: 'balls',\n",
       " 306: 'balsamic',\n",
       " 307: 'bamboo',\n",
       " 308: 'barely',\n",
       " 309: 'base',\n",
       " 310: 'based',\n",
       " 311: 'basic',\n",
       " 312: 'basil',\n",
       " 313: 'basmati',\n",
       " 314: 'bassett',\n",
       " 315: 'bassettcooking',\n",
       " 316: 'bassettrecipe',\n",
       " 317: 'baste',\n",
       " 318: 'baster',\n",
       " 319: 'basting',\n",
       " 320: 'batch',\n",
       " 321: 'batches',\n",
       " 322: 'bath',\n",
       " 323: 'batter',\n",
       " 324: 'battered',\n",
       " 325: 'bay',\n",
       " 326: 'bbq',\n",
       " 327: 'be',\n",
       " 328: 'bead',\n",
       " 329: 'bean',\n",
       " 330: 'beans',\n",
       " 331: 'beat',\n",
       " 332: 'beaten',\n",
       " 333: 'because',\n",
       " 334: 'become',\n",
       " 335: 'becomes',\n",
       " 336: 'beef',\n",
       " 337: 'been',\n",
       " 338: 'beer',\n",
       " 339: 'beet',\n",
       " 340: 'before',\n",
       " 341: 'beforehand',\n",
       " 342: 'begin',\n",
       " 343: 'beginning',\n",
       " 344: 'begins',\n",
       " 345: 'begun',\n",
       " 346: 'beijing',\n",
       " 347: 'being',\n",
       " 348: 'bell',\n",
       " 349: 'belly',\n",
       " 350: 'below',\n",
       " 351: 'best',\n",
       " 352: 'bestseller',\n",
       " 353: 'better',\n",
       " 354: 'between',\n",
       " 355: 'big',\n",
       " 356: 'biscuit',\n",
       " 357: 'bit',\n",
       " 358: 'bite',\n",
       " 359: 'bits',\n",
       " 360: 'black',\n",
       " 361: 'blacken',\n",
       " 362: 'blanch',\n",
       " 363: 'blanched',\n",
       " 364: 'blend',\n",
       " 365: 'blended',\n",
       " 366: 'blender',\n",
       " 367: 'blending',\n",
       " 368: 'bleu',\n",
       " 369: 'blog',\n",
       " 370: 'bloody',\n",
       " 371: 'blot',\n",
       " 372: 'blow',\n",
       " 373: 'blue',\n",
       " 374: 'blueberry',\n",
       " 375: 'board',\n",
       " 376: 'boil',\n",
       " 377: 'boiled',\n",
       " 378: 'boiling',\n",
       " 379: 'bok',\n",
       " 380: 'bon',\n",
       " 381: 'boneless',\n",
       " 382: 'bones',\n",
       " 383: 'booksdiana',\n",
       " 384: 'border',\n",
       " 385: 'boston',\n",
       " 386: 'both',\n",
       " 387: 'bottom',\n",
       " 388: 'bottomed',\n",
       " 389: 'bottoms',\n",
       " 390: 'bought',\n",
       " 391: 'bowl',\n",
       " 392: 'bowls',\n",
       " 393: 'bowlskitcheniq',\n",
       " 394: 'box',\n",
       " 395: 'boxadminprofileaccountlogoutsign',\n",
       " 396: 'boys',\n",
       " 397: 'brands',\n",
       " 398: 'break',\n",
       " 399: 'breaking',\n",
       " 400: 'breast',\n",
       " 401: 'breasts',\n",
       " 402: 'bride',\n",
       " 403: 'briefly',\n",
       " 404: 'bright',\n",
       " 405: 'brine',\n",
       " 406: 'bring',\n",
       " 407: 'bringing',\n",
       " 408: 'bringkettle',\n",
       " 409: 'brisket',\n",
       " 410: 'broadcast',\n",
       " 411: 'broccoli',\n",
       " 412: 'broccolini',\n",
       " 413: 'broil',\n",
       " 414: 'broiler',\n",
       " 415: 'broth',\n",
       " 416: 'brown',\n",
       " 417: 'browned',\n",
       " 418: 'browning',\n",
       " 419: 'browns',\n",
       " 420: 'brush',\n",
       " 421: 'brushed',\n",
       " 422: 'brussels',\n",
       " 423: 'bubble',\n",
       " 424: 'bubbled',\n",
       " 425: 'bubbles',\n",
       " 426: 'bubbling',\n",
       " 427: 'bubbly',\n",
       " 428: 'buds',\n",
       " 429: 'buffalo',\n",
       " 430: 'bun',\n",
       " 431: 'buns',\n",
       " 432: 'burger',\n",
       " 433: 'burn',\n",
       " 434: 'burner',\n",
       " 435: 'burnt',\n",
       " 436: 'but',\n",
       " 437: 'butter',\n",
       " 438: 'buttermilk1',\n",
       " 439: 'by',\n",
       " 440: 'c',\n",
       " 441: 'cabbage',\n",
       " 442: 'cabbages',\n",
       " 443: 'cake',\n",
       " 444: 'cakes',\n",
       " 445: 'calculated',\n",
       " 446: 'called',\n",
       " 447: 'calls',\n",
       " 448: 'calorie',\n",
       " 449: 'calories',\n",
       " 450: 'can',\n",
       " 451: 'candy',\n",
       " 452: 'canner',\n",
       " 453: 'canning',\n",
       " 454: 'canola',\n",
       " 455: 'cant',\n",
       " 456: 'cantonese',\n",
       " 457: 'caps',\n",
       " 458: 'capsicum',\n",
       " 459: 'caramelise',\n",
       " 460: 'caramelises',\n",
       " 461: 'caramelize',\n",
       " 462: 'carbohydrate',\n",
       " 463: 'carbohydrate11g1',\n",
       " 464: 'carbohydrates',\n",
       " 465: 'cardamom',\n",
       " 466: 'care',\n",
       " 467: 'careful',\n",
       " 468: 'carefully',\n",
       " 469: 'carrot',\n",
       " 470: 'carrots',\n",
       " 471: 'cashews',\n",
       " 472: 'casing',\n",
       " 473: 'casserole',\n",
       " 474: 'cast',\n",
       " 475: 'cauliflower',\n",
       " 476: 'cause',\n",
       " 477: 'caution',\n",
       " 478: 'cavity',\n",
       " 479: 'cayenne',\n",
       " 480: 'cbs',\n",
       " 481: 'celebrations',\n",
       " 482: 'celery',\n",
       " 483: 'center',\n",
       " 484: 'cereal',\n",
       " 485: 'challenging',\n",
       " 486: 'changes',\n",
       " 487: 'chard',\n",
       " 488: 'check',\n",
       " 489: 'cheddar',\n",
       " 490: 'cheese',\n",
       " 491: 'cheesecake',\n",
       " 492: 'cheeses',\n",
       " 493: 'chestnuts',\n",
       " 494: 'chicken',\n",
       " 495: 'chief',\n",
       " 496: 'chiffonade',\n",
       " 497: 'children',\n",
       " 498: 'chile',\n",
       " 499: 'chiles',\n",
       " 500: 'chili',\n",
       " 501: 'chilies',\n",
       " 502: 'chill',\n",
       " 503: 'chilled',\n",
       " 504: 'chilli',\n",
       " 505: 'chinese',\n",
       " 506: 'chips',\n",
       " 507: 'chives',\n",
       " 508: 'chocolate',\n",
       " 509: 'choice',\n",
       " 510: 'cholesterol',\n",
       " 511: 'chop',\n",
       " 512: 'chopped',\n",
       " 513: 'chopped1',\n",
       " 514: 'chopping',\n",
       " 515: 'chops',\n",
       " 516: 'chopstick',\n",
       " 517: 'chopsticks',\n",
       " 518: 'chorizo',\n",
       " 519: 'chow',\n",
       " 520: 'choy',\n",
       " 521: 'chunks',\n",
       " 522: 'chutney',\n",
       " 523: 'cider',\n",
       " 524: 'cigar',\n",
       " 525: 'cilantro',\n",
       " 526: 'cilantro3',\n",
       " 527: 'cinnamon',\n",
       " 528: 'circle',\n",
       " 529: 'circles',\n",
       " 530: 'city',\n",
       " 531: 'clarify',\n",
       " 532: 'clay',\n",
       " 533: 'clean',\n",
       " 534: 'clear',\n",
       " 535: 'cleaver',\n",
       " 536: 'click',\n",
       " 537: 'clings',\n",
       " 538: 'close',\n",
       " 539: 'closed',\n",
       " 540: 'closest',\n",
       " 541: 'cloth',\n",
       " 542: 'cloudy',\n",
       " 543: 'cloves',\n",
       " 544: 'clump',\n",
       " 545: 'clumps',\n",
       " 546: 'cm',\n",
       " 547: 'coarse',\n",
       " 548: 'coarsely',\n",
       " 549: 'coat',\n",
       " 550: 'coated',\n",
       " 551: 'coating',\n",
       " 552: 'coconut',\n",
       " 553: 'colander',\n",
       " 554: 'cold',\n",
       " 555: 'coleslaw',\n",
       " 556: 'collander',\n",
       " 557: 'collapsible',\n",
       " 558: 'collect',\n",
       " 559: 'color',\n",
       " 560: 'coloring',\n",
       " 561: 'com',\n",
       " 562: 'combination',\n",
       " 563: 'combine',\n",
       " 564: 'combined',\n",
       " 565: 'combines',\n",
       " 566: 'come',\n",
       " 567: 'comes',\n",
       " 568: 'coming',\n",
       " 569: 'common',\n",
       " 570: 'complete',\n",
       " 571: 'completely',\n",
       " 572: 'condé',\n",
       " 573: 'confections',\n",
       " 574: 'connect',\n",
       " 575: 'consistency',\n",
       " 576: 'constantly',\n",
       " 577: 'consult',\n",
       " 578: 'contact',\n",
       " 579: 'container',\n",
       " 580: 'continually',\n",
       " 581: 'continue',\n",
       " 582: 'continuing',\n",
       " 583: 'continuously',\n",
       " 584: 'contrary',\n",
       " 585: 'cook',\n",
       " 586: 'cookbook',\n",
       " 587: 'cookbookannual',\n",
       " 588: 'cooked',\n",
       " 589: 'cooker',\n",
       " 590: 'cookerplace',\n",
       " 591: 'cookie',\n",
       " 592: 'cookies',\n",
       " 593: 'cooking',\n",
       " 594: 'cookingdecember',\n",
       " 595: 'cookingjanuary',\n",
       " 596: 'cookingjuly',\n",
       " 597: 'cooks',\n",
       " 598: 'cookwaredreamfarm',\n",
       " 599: 'cool',\n",
       " 600: 'cooled',\n",
       " 601: 'cooling',\n",
       " 602: 'cools',\n",
       " 603: 'copyright',\n",
       " 604: 'core',\n",
       " 605: 'coriander',\n",
       " 606: 'corn',\n",
       " 607: 'corner',\n",
       " 608: 'corners',\n",
       " 609: 'cornflour',\n",
       " 610: 'corns',\n",
       " 611: 'cornstarch',\n",
       " 612: 'cough',\n",
       " 613: 'could',\n",
       " 614: 'country',\n",
       " 615: 'couple',\n",
       " 616: 'cover',\n",
       " 617: 'covered',\n",
       " 618: 'covering',\n",
       " 619: 'covers',\n",
       " 620: 'crab',\n",
       " 621: 'crack',\n",
       " 622: 'cracked',\n",
       " 623: 'crackly',\n",
       " 624: 'cranberry',\n",
       " 625: 'cream',\n",
       " 626: 'cream3',\n",
       " 627: 'creaminess',\n",
       " 628: 'creamy',\n",
       " 629: 'creases',\n",
       " 630: 'create',\n",
       " 631: 'creating',\n",
       " 632: 'crepe',\n",
       " 633: 'crepes',\n",
       " 634: 'creuset',\n",
       " 635: 'crisp',\n",
       " 636: 'crisped',\n",
       " 637: 'crisper',\n",
       " 638: 'crisps',\n",
       " 639: 'crispy',\n",
       " 640: 'crispyshallots',\n",
       " 641: 'crock',\n",
       " 642: 'crosswise',\n",
       " 643: 'crowd',\n",
       " 644: 'crowded',\n",
       " 645: 'crumble',\n",
       " 646: 'crumbly',\n",
       " 647: 'crunchy',\n",
       " 648: 'crushed',\n",
       " 649: 'crust',\n",
       " 650: 'crystal',\n",
       " 651: 'cubed1',\n",
       " 652: 'cubes',\n",
       " 653: 'cucumber',\n",
       " 654: 'culinary',\n",
       " 655: 'cumin',\n",
       " 656: 'cup',\n",
       " 657: 'cups',\n",
       " 658: 'curds',\n",
       " 659: 'currently',\n",
       " 660: 'curry',\n",
       " 661: 'cut',\n",
       " 662: 'cutter',\n",
       " 663: 'cutting',\n",
       " 664: 'cycle',\n",
       " 665: 'd',\n",
       " 666: 'dab',\n",
       " 667: 'daily',\n",
       " 668: 'damp',\n",
       " 669: 'dampened',\n",
       " 670: 'danziger',\n",
       " 671: 'dark',\n",
       " 672: 'darken',\n",
       " 673: 'darker',\n",
       " 674: 'dashes',\n",
       " 675: 'data',\n",
       " 676: 'day',\n",
       " 677: 'days',\n",
       " 678: 'decided',\n",
       " 679: 'deep',\n",
       " 680: 'deeply',\n",
       " 681: 'definitely',\n",
       " 682: 'defrost',\n",
       " 683: 'deglaze',\n",
       " 684: 'degree',\n",
       " 685: 'degrees',\n",
       " 686: 'degreesstart',\n",
       " 687: 'delicate',\n",
       " 688: 'delicious',\n",
       " 689: 'deliciousapril',\n",
       " 690: 'deliciousaugust',\n",
       " 691: 'deliver',\n",
       " 692: 'dent',\n",
       " 693: 'dente',\n",
       " 694: 'depending',\n",
       " 695: 'design',\n",
       " 696: 'desired',\n",
       " 697: 'dessert',\n",
       " 698: 'desserts',\n",
       " 699: 'diabetic',\n",
       " 700: 'diagonal',\n",
       " 701: 'diagonally',\n",
       " 702: 'diameter',\n",
       " 703: 'diamond',\n",
       " 704: 'diana',\n",
       " 705: 'dice',\n",
       " 706: 'diced',\n",
       " 707: 'diced1',\n",
       " 708: 'diet',\n",
       " 709: 'dietary',\n",
       " 710: 'different',\n",
       " 711: 'difficult',\n",
       " 712: 'dig',\n",
       " 713: 'dim',\n",
       " 714: 'diner',\n",
       " 715: 'dinner',\n",
       " 716: 'dip',\n",
       " 717: 'dipped',\n",
       " 718: 'dipping',\n",
       " 719: 'direct',\n",
       " 720: 'directed',\n",
       " 721: 'direction',\n",
       " 722: 'directions',\n",
       " 723: 'directionspour',\n",
       " 724: 'directly',\n",
       " 725: 'disc',\n",
       " 726: 'discard',\n",
       " 727: 'discarded',\n",
       " 728: 'discarding',\n",
       " 729: 'dish',\n",
       " 730: 'dishes',\n",
       " 731: 'disposable',\n",
       " 732: 'dissolve',\n",
       " 733: 'dissolved',\n",
       " 734: 'distribute',\n",
       " 735: 'divide',\n",
       " 736: 'do',\n",
       " 737: 'doctor',\n",
       " 738: 'does',\n",
       " 739: 'doesn',\n",
       " 740: 'doesnt',\n",
       " 741: 'dominent',\n",
       " 742: 'don',\n",
       " 743: 'done',\n",
       " 744: 'doneness',\n",
       " 745: 'dont',\n",
       " 746: 'double',\n",
       " 747: 'doubled',\n",
       " 748: 'dough',\n",
       " 749: 'doughnut',\n",
       " 750: 'doughnuts',\n",
       " 751: 'down',\n",
       " 752: 'downwards',\n",
       " 753: 'dozen',\n",
       " 754: 'drain',\n",
       " 755: 'drained',\n",
       " 756: 'dredge',\n",
       " 757: 'dredged',\n",
       " 758: 'dressing',\n",
       " 759: 'dried',\n",
       " 760: 'drippings',\n",
       " 761: 'drizzle',\n",
       " 762: 'drizzled',\n",
       " 763: 'drop',\n",
       " 764: 'dropped',\n",
       " 765: 'drops',\n",
       " 766: 'dry',\n",
       " 767: 'dryer',\n",
       " 768: 'drying',\n",
       " 769: 'duck',\n",
       " 770: 'dumpling',\n",
       " 771: 'dumplings',\n",
       " 772: 'dumplingsheat',\n",
       " 773: 'dumplingsif',\n",
       " 774: 'during',\n",
       " 775: 'dust',\n",
       " 776: 'dusted',\n",
       " 777: 'dutch',\n",
       " 778: 'e',\n",
       " 779: 'each',\n",
       " 780: 'earlier',\n",
       " 781: 'early',\n",
       " 782: 'easier',\n",
       " 783: 'easily',\n",
       " 784: 'eastern',\n",
       " 785: 'easy',\n",
       " 786: 'eat',\n",
       " 787: 'eaten',\n",
       " 788: 'eating',\n",
       " 789: 'eats',\n",
       " 790: 'edamame',\n",
       " 791: 'edge',\n",
       " 792: 'edges',\n",
       " 793: 'editor',\n",
       " 794: 'education',\n",
       " 795: 'effect',\n",
       " 796: 'egg',\n",
       " 797: 'eggplant',\n",
       " 798: 'eggroll',\n",
       " 799: 'eggrolls',\n",
       " 800: 'eggs',\n",
       " 801: 'eggshell',\n",
       " 802: 'eight',\n",
       " 803: 'either',\n",
       " 804: 'elastic',\n",
       " 805: 'electric',\n",
       " 806: 'eliminate',\n",
       " 807: 'else',\n",
       " 808: 'email',\n",
       " 809: 'empty',\n",
       " 810: 'emulsifiedalmost',\n",
       " 811: 'enameled',\n",
       " 812: 'enclosing',\n",
       " 813: 'end',\n",
       " 814: 'ends',\n",
       " 815: 'enjoy',\n",
       " 816: 'enjoying',\n",
       " 817: 'enough',\n",
       " 818: 'ensure',\n",
       " 819: 'entails',\n",
       " 820: 'entire',\n",
       " 821: 'entree',\n",
       " 822: 'epicuriousabout',\n",
       " 823: 'equal',\n",
       " 824: 'equals',\n",
       " 825: 'especially',\n",
       " 826: 'essentially',\n",
       " 827: 'estimated',\n",
       " 828: 'etc',\n",
       " 829: 'etched',\n",
       " 830: 'evaporate',\n",
       " 831: 'evaporated',\n",
       " 832: 'evaporates',\n",
       " 833: 'even',\n",
       " 834: 'evenly',\n",
       " 835: 'ever',\n",
       " 836: 'every',\n",
       " 837: 'everyday',\n",
       " 838: 'everything',\n",
       " 839: 'exactly',\n",
       " 840: 'except',\n",
       " 841: 'excess',\n",
       " 842: 'exchanges',\n",
       " 843: 'exhaust',\n",
       " 844: 'experiment',\n",
       " 845: 'extended',\n",
       " 846: 'extra',\n",
       " 847: 'extract',\n",
       " 848: 'extreme',\n",
       " 849: 'eye',\n",
       " 850: 'f',\n",
       " 851: 'face',\n",
       " 852: 'facebook',\n",
       " 853: 'facebooksign',\n",
       " 854: 'facing',\n",
       " 855: 'facts',\n",
       " 856: 'factsservings',\n",
       " 857: 'fajita',\n",
       " 858: 'fall',\n",
       " 859: 'family',\n",
       " 860: 'fan',\n",
       " 861: 'fanatic',\n",
       " 862: 'fanaticfood',\n",
       " 863: 'fanaticmy',\n",
       " 864: 'fanaticsign',\n",
       " 865: 'fancy',\n",
       " 866: 'fantastic',\n",
       " 867: 'farthest',\n",
       " 868: 'fashion',\n",
       " 869: 'fast',\n",
       " 870: 'faster',\n",
       " 871: 'fat',\n",
       " 872: 'fat3g9',\n",
       " 873: 'fat6g16',\n",
       " 874: 'fats',\n",
       " 875: 'favorite',\n",
       " 876: 'february',\n",
       " 877: 'feel',\n",
       " 878: 'fennel',\n",
       " 879: 'fermented',\n",
       " 880: 'few',\n",
       " 881: 'fiber',\n",
       " 882: 'fiber1g',\n",
       " 883: 'fifteen',\n",
       " 884: 'figure',\n",
       " 885: 'figuring',\n",
       " 886: 'fill',\n",
       " 887: 'filled',\n",
       " 888: 'fillet',\n",
       " 889: 'filling',\n",
       " 890: 'fillingheat',\n",
       " 891: 'fillings',\n",
       " 892: 'fills',\n",
       " 893: 'filter',\n",
       " 894: 'final',\n",
       " 895: 'finally',\n",
       " 896: 'find',\n",
       " 897: 'finding',\n",
       " 898: 'fine',\n",
       " 899: 'finely',\n",
       " 900: 'finger',\n",
       " 901: 'fingers',\n",
       " 902: 'fingertip',\n",
       " 903: 'fingertips',\n",
       " 904: 'finish',\n",
       " 905: 'finished',\n",
       " 906: 'finishing',\n",
       " 907: 'fire',\n",
       " 908: 'firm',\n",
       " 909: 'firmed',\n",
       " 910: 'firmly',\n",
       " 911: 'first',\n",
       " 912: 'fish',\n",
       " 913: 'fit',\n",
       " 914: 'fits',\n",
       " 915: 'fitting',\n",
       " 916: 'five',\n",
       " 917: 'flakes',\n",
       " 918: 'flame',\n",
       " 919: 'flat',\n",
       " 920: 'flatten',\n",
       " 921: 'flavor',\n",
       " 922: 'flavored',\n",
       " 923: 'flavorful',\n",
       " 924: 'flavors',\n",
       " 925: 'flavours',\n",
       " 926: 'flesh',\n",
       " 927: 'fleshed',\n",
       " 928: 'fleshy',\n",
       " 929: 'flick',\n",
       " 930: 'flip',\n",
       " 931: 'flipping',\n",
       " 932: 'float',\n",
       " 933: 'floating',\n",
       " 934: 'flour',\n",
       " 935: 'floured',\n",
       " 936: 'flours',\n",
       " 937: 'fluff',\n",
       " 938: 'fluffed',\n",
       " 939: 'fluffy',\n",
       " 940: 'flush',\n",
       " 941: 'foaming',\n",
       " 942: 'foamy',\n",
       " 943: 'foil',\n",
       " 944: 'fold',\n",
       " 945: 'folding',\n",
       " 946: 'folds',\n",
       " 947: 'follow',\n",
       " 948: 'followed',\n",
       " 949: 'following',\n",
       " 950: 'food',\n",
       " 951: 'foodie',\n",
       " 952: 'foodrelated',\n",
       " 953: 'foods',\n",
       " 954: 'foolproof',\n",
       " 955: 'for',\n",
       " 956: 'forfree',\n",
       " 957: 'fork',\n",
       " 958: 'forks',\n",
       " 959: 'form',\n",
       " 960: 'forms',\n",
       " 961: 'forth',\n",
       " 962: 'forward',\n",
       " 963: 'found',\n",
       " 964: 'four',\n",
       " 965: 'fragrance',\n",
       " 966: 'fragrant',\n",
       " 967: 'franks',\n",
       " 968: 'free',\n",
       " 969: 'freeze',\n",
       " 970: 'freezer',\n",
       " 971: 'freezing',\n",
       " 972: 'french',\n",
       " 973: 'frequently',\n",
       " 974: 'fresh',\n",
       " 975: 'fridge',\n",
       " 976: 'fried',\n",
       " 977: 'friedcategory',\n",
       " 978: 'friend',\n",
       " 979: 'friendly',\n",
       " 980: 'fries',\n",
       " 981: 'friesavocados',\n",
       " 982: 'from',\n",
       " 983: 'front',\n",
       " 984: 'frozen',\n",
       " 985: 'fry',\n",
       " 986: 'fryer',\n",
       " 987: 'frying',\n",
       " 988: 'frypan',\n",
       " 989: 'full',\n",
       " 990: 'fully',\n",
       " 991: 'fun',\n",
       " 992: 'further',\n",
       " 993: 'future',\n",
       " 994: 'g',\n",
       " 995: 'gallon',\n",
       " 996: 'gap',\n",
       " 997: 'garlic',\n",
       " 998: 'garnish',\n",
       " 999: 'garnished',\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see those same characters from above, encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 661, 1215, 1257, 1136, 1068, 1264,  234, 2280,  661,  642, 1181,\n",
       "       2429, 2296, 1841,   11,  496,   14, 2149, 1215,  955,  152, 1385,\n",
       "       1343,   18, 1096, 2421, 1472, 1136, 1241, 1894, 1510, 1353,   19,\n",
       "       1306, 1096,  191,  997,  234,  585,   18, 2171,   18,  955,   64,\n",
       "       1385,   18,  347,  467, 1449, 2324,  416,  997, 1742, 1096, 2324,\n",
       "       1353,  234,  191, 2150, 1215,  234, 1909,  585,  955,   64, 1385,\n",
       "        234, 2280,  191, 1843,  234,  585,  955,  237,   64, 1385,   18,\n",
       "       2171,  191, 2087, 1893,  234,  585,  955,  105, 1935, 1404, 1096,\n",
       "       2507, 1510, 1353, 1105, 1096,  191, 1472,  167, 2280,  191, 2375,\n",
       "        234])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(idx2word)\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../working/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkDataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    This is the class for construct data generator which build data batches for model training\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sentences, sequence, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize the data generator\n",
    "        \"\"\"\n",
    "\n",
    "        self.sentences = sentences\n",
    "        self.sequence = sequence\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Denotes the number of batches per epoch\n",
    "        \"\"\"\n",
    "\n",
    "        number_of_batches = int(np.ceil(len(self.sentences)) / self.batch_size)\n",
    "        return number_of_batches\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generate one batch of data\n",
    "\n",
    "        :param index (int): The batch index for slicing\n",
    "        :return tuple (batch_data, batch_label): Tuple of data and labels\n",
    "        \"\"\"\n",
    "        sequence = 10\n",
    "\n",
    "        sentences = []\n",
    "        next_chars = []\n",
    "\n",
    "\n",
    "        for i in range(0, len(corpus) - sequence):\n",
    " \n",
    "            sentences.append(corpus[i: i + sequence])\n",
    "            next_chars.append(corpus[i + sequence])\n",
    "\n",
    "\n",
    "        number_of_sentences = len(sentences)\n",
    "        starting_sentence_index = index * self.batch_size\n",
    "        ending_sentence_index = ((index + 1) * (self.batch_size)) - self.sequence\n",
    "        \n",
    "        data = np.zeros((self.batch_size, self.sequence, vocab_size))\n",
    "        labels = np.zeros((self.batch_size, vocab_size))\n",
    "        \n",
    "        for i, sentence in enumerate(self.sentences[starting_sentence_index : ending_sentence_index]):\n",
    "            for t, char in enumerate(sentence):\n",
    "                data[i, t, word2idx[char]] = 1\n",
    "            labels[i, word2idx[next_chars[i]]] = 1\n",
    "        \n",
    "        return (data, labels)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(chars,sequence):\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    print('Creating sentences for model')\n",
    "    progress = tqdm(total = len(corpus) - sequence)\n",
    "    for i in range(0, len(corpus) - sequence):\n",
    "        progress.update(1)\n",
    "        sentences.append(corpus[i: i + sequence])\n",
    "        next_chars.append(corpus[i + sequence])\n",
    "    progress.close()\n",
    "    number_of_sentences = len(sentences)\n",
    "    \n",
    "    print('Creating Generator')\n",
    "    network_generator = NetworkDataGenerator(sentences = sentences, sequence= sequence, \n",
    "                                             batch_size = batch_size)\n",
    "    \n",
    "    model = Sequential()\n",
    "    embedding_dim = 512\n",
    "    #model.add(keras.layers.SimpleRNN(embedding_dim, input_shape=(sequence, vocab_size)))\n",
    "    model.add(keras.layers.LSTM(embedding_dim, input_shape=(sequence, vocab_size)))\n",
    "    model.add(Dense(len(chars)))\n",
    "    model.add(Activation('softmax'))\n",
    "    optimizer = RMSprop(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return model, network_generator\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(filepath, model, network_generator):\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
    "                                monitor = 'val_loss',\n",
    "                                verbose = 0,\n",
    "                                save_best_only = True,\n",
    "                                save_weights_only = False,\n",
    "                                mode = 'auto',\n",
    "                                period = 1)\n",
    "    \n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    history = model.fit_generator(generator = network_generator, steps_per_epoch = len(network_generator), \n",
    "                                  epochs=100, callbacks=callbacks_list)\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    print(\"Training Time : %8.4f (s)\" % (duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38272/38272 [00:00<00:00, 484357.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sentences for model\n",
      "Creating Generator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 512)               6322176   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2574)              1320462   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2574)              0         \n",
      "=================================================================\n",
      "Total params: 7,642,638\n",
      "Trainable params: 7,642,638\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "chinese_word_model, rnn_10_network_generator = lstm(idx2word,10)\n",
    "chinese_word_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "74/74 [==============================] - 15s 202ms/step - loss: 5.0397\n",
      "Epoch 2/100\n",
      " 1/74 [..............................] - ETA: 11s - loss: 4.7146"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 13s 179ms/step - loss: 4.7128\n",
      "Epoch 3/100\n",
      "74/74 [==============================] - 14s 191ms/step - loss: 4.8410\n",
      "Epoch 4/100\n",
      "74/74 [==============================] - 13s 181ms/step - loss: 4.6830\n",
      "Epoch 5/100\n",
      "74/74 [==============================] - 14s 185ms/step - loss: 4.6767\n",
      "Epoch 6/100\n",
      "74/74 [==============================] - 13s 182ms/step - loss: 4.5739\n",
      "Epoch 7/100\n",
      "74/74 [==============================] - 14s 185ms/step - loss: 4.3837\n",
      "Epoch 8/100\n",
      " 2/74 [..............................] - ETA: 17s - loss: 4.2067"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.203413). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 13s 181ms/step - loss: 4.1804\n",
      "Epoch 9/100\n",
      "74/74 [==============================] - 14s 186ms/step - loss: 3.8992\n",
      "Epoch 10/100\n",
      "74/74 [==============================] - 14s 184ms/step - loss: 3.5855\n",
      "Epoch 11/100\n",
      " 2/74 [..............................] - ETA: 19s - loss: 3.1130"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.232954). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 14s 184ms/step - loss: 3.1813\n",
      "Epoch 12/100\n",
      "74/74 [==============================] - 14s 184ms/step - loss: 2.8593\n",
      "Epoch 13/100\n",
      "74/74 [==============================] - 13s 182ms/step - loss: 2.5012\n",
      "Epoch 14/100\n",
      "74/74 [==============================] - 14s 185ms/step - loss: 2.1095\n",
      "Epoch 15/100\n",
      " 2/74 [..............................] - ETA: 21s - loss: 1.7475"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.211397). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 14s 184ms/step - loss: 1.7834\n",
      "Epoch 16/100\n",
      "74/74 [==============================] - 13s 181ms/step - loss: 1.4560\n",
      "Epoch 17/100\n",
      "74/74 [==============================] - 14s 185ms/step - loss: 1.1536\n",
      "Epoch 18/100\n",
      " 6/74 [=>............................] - ETA: 20s - loss: 0.6677"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.211376). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 13s 182ms/step - loss: 0.9165\n",
      "Epoch 19/100\n",
      "74/74 [==============================] - 13s 181ms/step - loss: 0.7319\n",
      "Epoch 20/100\n",
      "74/74 [==============================] - 13s 182ms/step - loss: 0.6136\n",
      "Epoch 21/100\n",
      "74/74 [==============================] - 14s 186ms/step - loss: 0.5014\n",
      "Epoch 22/100\n",
      "74/74 [==============================] - 14s 184ms/step - loss: 0.3929\n",
      "Epoch 23/100\n",
      "74/74 [==============================] - 13s 181ms/step - loss: 0.5287\n",
      "Epoch 24/100\n",
      "74/74 [==============================] - 14s 192ms/step - loss: 0.3717\n",
      "Epoch 25/100\n",
      "74/74 [==============================] - 13s 180ms/step - loss: 0.3121\n",
      "Epoch 26/100\n",
      "74/74 [==============================] - 14s 184ms/step - loss: 0.2843\n",
      "Epoch 27/100\n",
      "74/74 [==============================] - 13s 182ms/step - loss: 0.2937\n",
      "Epoch 28/100\n",
      "74/74 [==============================] - 14s 185ms/step - loss: 0.2567\n",
      "Epoch 29/100\n",
      "74/74 [==============================] - 13s 182ms/step - loss: 0.2704\n",
      "Epoch 30/100\n",
      "74/74 [==============================] - 13s 181ms/step - loss: 0.2402\n",
      "Epoch 31/100\n",
      "74/74 [==============================] - 14s 184ms/step - loss: 0.2244\n",
      "Epoch 32/100\n",
      "74/74 [==============================] - 13s 181ms/step - loss: 0.2268\n",
      "Epoch 33/100\n",
      "74/74 [==============================] - 13s 181ms/step - loss: 0.2244\n",
      "Epoch 34/100\n",
      "74/74 [==============================] - 13s 182ms/step - loss: 0.2194\n",
      "Epoch 35/100\n",
      "74/74 [==============================] - 14s 185ms/step - loss: 0.2163\n",
      "Epoch 36/100\n",
      "74/74 [==============================] - 14s 182ms/step - loss: 0.1696\n",
      "Epoch 37/100\n",
      "74/74 [==============================] - 14s 182ms/step - loss: 0.1906\n",
      "Epoch 38/100\n",
      "74/74 [==============================] - 14s 184ms/step - loss: 0.2541\n",
      "Epoch 39/100\n",
      "74/74 [==============================] - 13s 182ms/step - loss: 0.1760\n",
      "Epoch 40/100\n",
      "74/74 [==============================] - 14s 186ms/step - loss: 0.4174\n",
      "Epoch 41/100\n",
      "74/74 [==============================] - 13s 182ms/step - loss: 0.1652\n",
      "Epoch 42/100\n",
      "74/74 [==============================] - 14s 185ms/step - loss: 0.1890\n",
      "Epoch 43/100\n",
      "74/74 [==============================] - 14s 185ms/step - loss: 0.1771\n",
      "Epoch 44/100\n",
      "74/74 [==============================] - 13s 181ms/step - loss: 0.1633\n",
      "Epoch 45/100\n",
      "74/74 [==============================] - 14s 183ms/step - loss: 0.1405\n",
      "Epoch 46/100\n",
      "74/74 [==============================] - 14s 189ms/step - loss: 0.1583\n",
      "Epoch 47/100\n",
      "74/74 [==============================] - 14s 184ms/step - loss: 0.1622\n",
      "Epoch 48/100\n",
      "74/74 [==============================] - 13s 180ms/step - loss: 0.1490\n",
      "Epoch 49/100\n",
      "74/74 [==============================] - 13s 181ms/step - loss: 0.1518\n",
      "Epoch 50/100\n",
      "74/74 [==============================] - 14s 184ms/step - loss: 0.1557\n",
      "Epoch 51/100\n",
      "74/74 [==============================] - 13s 180ms/step - loss: 0.1584\n",
      "Epoch 52/100\n",
      "74/74 [==============================] - 14s 183ms/step - loss: 0.1552\n",
      "Epoch 53/100\n",
      "74/74 [==============================] - 13s 179ms/step - loss: 0.1508\n",
      "Epoch 54/100\n",
      "74/74 [==============================] - 14s 183ms/step - loss: 0.1443\n",
      "Epoch 55/100\n",
      "74/74 [==============================] - 13s 180ms/step - loss: 0.1539\n",
      "Epoch 56/100\n",
      "74/74 [==============================] - 14s 184ms/step - loss: 0.1479\n",
      "Epoch 57/100\n",
      "74/74 [==============================] - 14s 183ms/step - loss: 0.1453\n",
      "Epoch 58/100\n",
      "74/74 [==============================] - 14s 184ms/step - loss: 0.1505\n",
      "Epoch 59/100\n",
      "74/74 [==============================] - 13s 181ms/step - loss: 0.1446\n",
      "Epoch 60/100\n",
      "74/74 [==============================] - 13s 180ms/step - loss: 0.1317\n",
      "Epoch 61/100\n",
      "74/74 [==============================] - 13s 182ms/step - loss: 0.1298\n",
      "Epoch 62/100\n",
      "74/74 [==============================] - 14s 184ms/step - loss: 0.1513\n",
      "Epoch 63/100\n",
      " 2/74 [..............................] - ETA: 19s - loss: 0.0620"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.206430). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 14s 183ms/step - loss: 0.1517\n",
      "Epoch 64/100\n",
      "74/74 [==============================] - 13s 182ms/step - loss: 0.1534\n",
      "Epoch 65/100\n",
      "74/74 [==============================] - 13s 182ms/step - loss: 0.3486\n",
      "Epoch 66/100\n",
      "74/74 [==============================] - 14s 183ms/step - loss: 0.9629\n",
      "Epoch 67/100\n",
      "74/74 [==============================] - 13s 181ms/step - loss: 0.5639\n",
      "Epoch 68/100\n",
      "74/74 [==============================] - 14s 189ms/step - loss: 0.4884\n",
      "Epoch 69/100\n",
      "74/74 [==============================] - 14s 184ms/step - loss: 0.2993\n",
      "Epoch 70/100\n",
      "74/74 [==============================] - 13s 181ms/step - loss: 0.2892\n",
      "Epoch 71/100\n",
      "74/74 [==============================] - 14s 183ms/step - loss: 0.1932\n",
      "Epoch 72/100\n",
      " 2/74 [..............................] - ETA: 20s - loss: 0.1490"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.234972). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 13s 180ms/step - loss: 0.1737\n",
      "Epoch 73/100\n",
      "74/74 [==============================] - 14s 184ms/step - loss: 0.1534\n",
      "Epoch 74/100\n",
      "74/74 [==============================] - 14s 185ms/step - loss: 0.1576\n",
      "Epoch 75/100\n",
      "74/74 [==============================] - 13s 181ms/step - loss: 0.1788\n",
      "Epoch 76/100\n",
      "74/74 [==============================] - 14s 183ms/step - loss: 0.1565\n",
      "Epoch 77/100\n",
      "74/74 [==============================] - 13s 182ms/step - loss: 0.1576\n",
      "Epoch 78/100\n",
      "74/74 [==============================] - 14s 182ms/step - loss: 0.1568\n",
      "Epoch 79/100\n",
      "74/74 [==============================] - 14s 183ms/step - loss: 0.2303\n",
      "Epoch 80/100\n",
      "74/74 [==============================] - 13s 182ms/step - loss: 0.2341\n",
      "Epoch 81/100\n",
      "74/74 [==============================] - 14s 183ms/step - loss: 0.3375\n",
      "Epoch 82/100\n",
      "74/74 [==============================] - 13s 180ms/step - loss: 0.3336\n",
      "Epoch 83/100\n",
      "74/74 [==============================] - 13s 182ms/step - loss: 0.2798\n",
      "Epoch 84/100\n",
      "74/74 [==============================] - 13s 182ms/step - loss: 0.2449\n",
      "Epoch 85/100\n",
      "74/74 [==============================] - 14s 190ms/step - loss: 0.2258\n",
      "Epoch 86/100\n",
      "74/74 [==============================] - 14s 190ms/step - loss: 0.2453\n",
      "Epoch 87/100\n",
      "74/74 [==============================] - 14s 190ms/step - loss: 0.2277\n",
      "Epoch 88/100\n",
      "74/74 [==============================] - 13s 181ms/step - loss: 0.2342\n",
      "Epoch 89/100\n",
      " 4/74 [>.............................] - ETA: 21s - loss: 0.1996"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.223332). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 14s 184ms/step - loss: 0.2130\n",
      "Epoch 90/100\n",
      "74/74 [==============================] - 14s 190ms/step - loss: 0.1657\n",
      "Epoch 91/100\n",
      " 2/74 [..............................] - ETA: 20s - loss: 0.0342"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.255947). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 15s 196ms/step - loss: 0.1165\n",
      "Epoch 92/100\n",
      "74/74 [==============================] - 14s 186ms/step - loss: 0.1417\n",
      "Epoch 93/100\n",
      "74/74 [==============================] - 13s 179ms/step - loss: 0.1210\n",
      "Epoch 94/100\n",
      "74/74 [==============================] - 14s 185ms/step - loss: 0.1323\n",
      "Epoch 95/100\n",
      "74/74 [==============================] - 14s 183ms/step - loss: 0.1194\n",
      "Epoch 96/100\n",
      "74/74 [==============================] - 13s 181ms/step - loss: 0.1174\n",
      "Epoch 97/100\n",
      "74/74 [==============================] - 14s 183ms/step - loss: 0.1183\n",
      "Epoch 98/100\n",
      "74/74 [==============================] - 13s 182ms/step - loss: 0.1230\n",
      "Epoch 99/100\n",
      "74/74 [==============================] - 13s 182ms/step - loss: 0.1201\n",
      "Epoch 100/100\n",
      " 2/74 [..............................] - ETA: 17s - loss: 0.2008"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.204100). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 13s 177ms/step - loss: 0.1118\n",
      "Training Time : 1361.3669 (s)\n"
     ]
    }
   ],
   "source": [
    "fit_model('chinese_word.hdf5',chinese_word_model, rnn_10_network_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_model = tf.keras.models.load_model(\"chinese_word.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebuild the model by checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# # from tensorflow.keras.models import load_model\n",
    "# model = tf.keras.models.load_model(\"/kaggle/working/chinese_word.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, length, diversity,sequence, starting_string):\n",
    "    starting_string = WordPunctTokenizer().tokenize(starting_string.lower())\n",
    " \n",
    "    sentence = starting_string[-10:]\n",
    "\n",
    "    generated = ''\n",
    "    generated = generated + ' '.join(sentence)\n",
    "   \n",
    "    for i in range(length):\n",
    "        \n",
    "            x_pred = np.zeros((1, sequence, len(vocab)+1))\n",
    "    \n",
    "\n",
    "            for t, char in enumerate(sentence):\n",
    "                #print('sentence',sentence)\n",
    "                \n",
    "                x_pred[0, t-1, word2idx[char]] = 1.\n",
    "     \n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "    \n",
    "            next_char = idx2word[next_index]\n",
    "\n",
    "\n",
    "            generated = generated + ' '+ str(next_char) \n",
    "    \n",
    "            sentence = sentence[1:] + [next_char]\n",
    "            #print('next_char',next_char)\n",
    "            #print('sentence', sentence)\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is heated through and peas are just cooked serve with for to , 2 strips 1 pepper pan : stirring cooked in size pat , cooked cilantro cook for stirring in add , dry large cooked and of nonstick large steam over high heat how sauce rice 1 oil and ingredients salt of steam - over - serve another , together : until ingredients salt of fry bright oils heat add if in add salt to 1 over or until toss of beef turn are rice sauté temperature for wok steak marinade , seconds cook for seconds more toss let rest then just room coat , immediately container , whisk\n"
     ]
    }
   ],
   "source": [
    "# [ 62 114 187  46  95]\n",
    "\n",
    "seed_everything(62)\n",
    "test_text = generate_text(chinese_word_model,100,0.2,10,starting_string=\"cooking until rice is heated through and peas are just cooked Serve with\")\n",
    "print(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is heated through and peas are just cooked serve with for to , 2 strips 1 pepper pan : stirring cooked in size pat , cooked cilantro cook for stirring in add , dry large cooked and of nonstick large steam over high heat how sauce rice 1 oil and ingredients salt of steam - over - serve another , together : until ingredients salt of fry bright oils heat add if in add salt to 1 over or until toss of beef turn are rice sauté temperature for wok steak marinade , seconds cook for seconds more toss let rest then just room coat , immediately container , whisk'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}