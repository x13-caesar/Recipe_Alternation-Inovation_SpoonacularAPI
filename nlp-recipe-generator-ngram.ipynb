{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import csv\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import operator\n",
    "import openpyxl\n",
    "import xlrd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import xlsxwriter\n",
    "import urllib.request\n",
    "from nltk.tokenize.regexp import (\n",
    "    RegexpTokenizer,\n",
    "    WhitespaceTokenizer,\n",
    "    BlanklineTokenizer,\n",
    "    WordPunctTokenizer,\n",
    "    wordpunct_tokenize,\n",
    "    regexp_tokenize,\n",
    "    blankline_tokenize,\n",
    ")\n",
    "\n",
    "from nltk.util import ngrams, pad_sequence\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/nlp_finalproj_data_withids.csv').iloc[:,0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_text(df,cuisine):\n",
    "    \n",
    "    df.drop_duplicates(subset = ['r_id'],keep = 'first',inplace = True)\n",
    "    df = df[df['cuisine']==cuisine]\n",
    "    docs = [str(instruction).replace('\\n','').replace('\\xa0','') for instruction in df['instructions']]\n",
    "    text = ''.join(docs)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create N-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramModel():\n",
    "    def __init__(self, n):\n",
    "        \"\"\"\n",
    "        n is the order of the ngram model\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.nm = n-1\n",
    "        \n",
    "        self.ngrams = []\n",
    "        self.tokens = []     \n",
    "        self.nmgrams = []\n",
    "        \n",
    "        # Initialize empty vocab\n",
    "        self.vocab = set([])\n",
    "        \n",
    "    def update(self, text): \n",
    "        \"\"\"\n",
    "        computes the n-grams for the input text and updates\n",
    "        the internal information. The input text is padded\n",
    "        with '.' as the prefix\n",
    "        \"\"\"\n",
    "        tokens = WordPunctTokenizer().tokenize(text.lower()) \n",
    "        \n",
    "        ngram = ngrams(tokens,self.n,pad_left=True, pad_right=False, left_pad_symbol='.')\n",
    "        nmgram = ngrams(tokens,self.n -1,pad_left=True, pad_right=False, left_pad_symbol='.')\n",
    "        \n",
    "        self.ngrams.extend(ngram)\n",
    "        self.nmgrams.extend(nmgram)\n",
    "        self.tokens.extend(tokens)\n",
    "        self.vocab = set(self.tokens)\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        \"\"\"\n",
    "        get all vocab used by this model\n",
    "        \"\"\"\n",
    "        return self.vocab\n",
    "    \n",
    "    def size_vocab(self):\n",
    "        \"\"\"\n",
    "        reture size of the vocab\n",
    "        \n",
    "        \"\"\"\n",
    "        # doesn't include padding\n",
    "        return len(self.vocab)-self.n+1\n",
    "               \n",
    "    def prob(self,context,word):\n",
    "        \"\"\"\n",
    "        accepts an (n-1)-length word string representing\n",
    "        a context,a word and returns the probability of that\n",
    "        word occurring, given the preceding context. it address\n",
    "        unseen problem\n",
    "        \"\"\"   \n",
    "        context = tuple(WordPunctTokenizer().tokenize(context.lower()))\n",
    "        word = word.lower()\n",
    "        vocal = self.get_vocab()\n",
    "        vocal_size = self.size_vocab()\n",
    "        # unigram \n",
    "        if self.n == 1:\n",
    "            word_count_in_tokens = self.tokens.count(word)\n",
    "            # if new word\n",
    "            if word_count_in_tokens == 0:        \n",
    "                prob = 1/(vocal_size + 1)\n",
    "            else:\n",
    "                prob = (word_count_in_tokens + 1) / (len(self.ngrams) + vocal_size )  \n",
    "        #n-gram for n > 1\n",
    "        else: \n",
    "            # if context is new\n",
    "            if context not in self.nmgrams:           \n",
    "                prob = 1/ vocal_size\n",
    "            # if context is old, but word is new\n",
    "            elif word not in vocal: \n",
    "                prob = 1/(vocal_size + 1)\n",
    "            # context is old and word is old\n",
    "            else: \n",
    "                temp = list(context)\n",
    "                temp.append(word)  \n",
    "                temp2 = list(temp)\n",
    "                context_word = tuple(temp2)     \n",
    "                prob = (self.ngrams.count(context_word)+1)/(self.nmgrams.count(context)+ vocal_size)\n",
    "                 \n",
    "        return prob\n",
    "        \n",
    "    def len_text(self):\n",
    "        return len(self.tokens)\n",
    "    \n",
    "    def len_ngram(self):\n",
    "        return len(self.ngrams)\n",
    "    \n",
    "    def word_freq(self, word):\n",
    "        cnt = 0\n",
    "        vocal_size = self.size_vocab()\n",
    "        for token in self.tokens:\n",
    "            if word == token:\n",
    "                cnt = cnt + 1\n",
    "        freq = (cnt +1)/(len(self.tokens) + vocal_size)\n",
    "        \n",
    "        if word not in set(self.tokens):\n",
    "            freq = 1/(1 + vocal_size)\n",
    "        \n",
    "        return freq\n",
    "        \n",
    "    def ngram_freq(self, gram):\n",
    "        cnt = 0\n",
    "        vocal_size = self.size_vocab()\n",
    "        for ngram in self.ngrams:\n",
    "            if gram == ngram:\n",
    "                cnt = cnt + 1\n",
    "        freq = (cnt + 1)/(len(self.ngrams) + vocal_size)\n",
    "        \n",
    "        if gram not in set(self.ngrams):\n",
    "           \n",
    "            freq = 1/vocal_size\n",
    "        return freq\n",
    "    \n",
    "    def generate_text(self, context, min_length, max_length):\n",
    "        \"\"\"\n",
    "        This function utlize n-gram model to generate sentences in a way that the probability of each n-gram\n",
    "        is according to the n-gram frequency in the n-gram model\n",
    "        \n",
    "        \"\"\"\n",
    "        random.seed(42)\n",
    "         \n",
    "        sentence_tokens = WordPunctTokenizer().tokenize(context.lower()) # tokens\n",
    "        sentence_length = len(sentence_tokens)\n",
    "        \n",
    "        while (sentence_length <= max_length): \n",
    "            if sentence_length < self.n-1:\n",
    "                selected_word = random.choice(self.tokens)\n",
    "            else:\n",
    "                # ngram tokens\n",
    "                cur_context = sentence_tokens[-self.n+1:]\n",
    "  \n",
    "                # convert to string\n",
    "                cur_context_str = \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in cur_context]).strip()\n",
    "\n",
    "                freq_table = {}\n",
    "                for word in self.get_vocab():\n",
    "                    frequency = self.prob(cur_context_str, word)\n",
    "                    freq_table[word] = frequency\n",
    "\n",
    "                sorted_freq_table =  sorted(freq_table.items(), key=operator.itemgetter(1, 0), reverse=True)\n",
    "               # check--in case the generator gets trapped in a loop\n",
    "                selected_word = None\n",
    "                for word_with_freq in sorted_freq_table:\n",
    "                    if word_with_freq[0] not in cur_context:\n",
    "                        selected_word = word_with_freq[0]\n",
    "                        break\n",
    "                if selected_word is None:\n",
    "                    selected_word = random.choice(word_with_freq)[0][0]\n",
    "\n",
    "            sentence_tokens.append(selected_word)\n",
    "            sentence_length = sentence_length + 1\n",
    "        \n",
    "        string_full_text = \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in sentence_tokens]).strip()      \n",
    "        return string_full_text\n",
    "\n",
    "    def perplexity(self, text):\n",
    "        \"\"\"\n",
    "        calculate perplexity of a given text with  n-gram model\n",
    "        \n",
    "        \"\"\"      \n",
    "        text = '. '*(self.n-1) + text\n",
    "        text_tokens = WordPunctTokenizer().tokenize(text.lower())\n",
    "    \n",
    "        inverse_prob_chain = 1\n",
    "        for i in range(1,len(text_tokens)-self.n+2):\n",
    "            word = text_tokens[-i]   \n",
    "            context_token = text_tokens[-self.n-i+1:-i]\n",
    "            context_str = \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in context_token]).strip()\n",
    "                    \n",
    "            prob = self.prob(context_str, word)\n",
    "            inverse_prob_chain = inverse_prob_chain * (1/prob)        \n",
    "       \n",
    "        perplexity = (inverse_prob_chain)**(1/(len(text_tokens)-self.n+1))\n",
    "\n",
    "        return perplexity\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size 2605\n",
      "heat the water and moisten all four edges of the wonton wrapper. fold the wrapper. fold the wrapper. fold the wrapper. fold the wrapper. fold the wrapper. fold the wrapper. fold the wrapper. fold the wrapper. fold the wrapper. fold the\n"
     ]
    }
   ],
   "source": [
    "#cuisine_lst = ['Chinese','Thai','American','Italian','Indian','Mediterranean','French']#\n",
    "\n",
    "#for cuisine in cuisine_lst:\n",
    "\n",
    "model = NgramModel(3)\n",
    "model.update(get_text(df,'Chinese'))\n",
    "print('vocab size',len(model.vocab))\n",
    "print(model.generate_text('Heat the water',0,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}